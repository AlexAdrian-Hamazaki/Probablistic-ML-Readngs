{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Distribution\n",
    "\n",
    "This is how we represent a distribution over a finite set of labels, with one parameter (y/n) per class/label $y \\in \\{1,...,C\\}$\n",
    "\n",
    "Generalizes the Bernouli to C>2 values\n",
    "\n",
    "Discrete probability distribution\n",
    "\n",
    "$Cat(y|\\theta) = \\prod_{c=1}^C \\theta_C^{\\prod(y=c)}$\n",
    "\n",
    "$\\theta = some probability$\n",
    "\n",
    "This formula just means that the joint discrete probability distribution is a multiplication of the probabilities of the different category levels\n",
    "\n",
    "### One Hot Encoding\n",
    "\n",
    "This can be represented as one-hot encoded as well\n",
    "\n",
    "$Cat(y|\\theta) = \\prod_{c=1}^C \\theta_C^{y_c}$\n",
    "\n",
    "The probability of having a given y=x is taken by multiplying the \n",
    "\n",
    "# Multinomial distribution\n",
    "\n",
    "This is how we can represent the probability distribution over a finite set of labels, when each label can have >2 possible values\n",
    "\n",
    "Example, rolling 6 sided dice N times\n",
    "\n",
    "$y_n \\sim Cat(*|\\theta)$ for n=1:N\n",
    "\n",
    "This gives us the a vector of length 6, and it would be a uniform distribution\n",
    "\n",
    "This is called a multi-hot distribution.\n",
    "\n",
    "$\\omega(y|N, \\theta) = \\binom{N}{y_1...y_C} \\prod_{c=1}^C \\theta_C^{y_c} = \\binom{N}{N_1...N_C} \\prod_{c=1}^C \\theta_C^{N_c}$\n",
    "\n",
    "Where $\\theta_c$ is the probability that occurance c occurs.\n",
    "\n",
    "Where $\\binom{N}{N_1...N_C}$ is the multinomical coefficient. This is the number of ways to divide a set of size $N_c$ into subsets with sizes $N_1$ up to $N_C$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Function\n",
    "\n",
    "Again, in the case of a conditional multinomial/categorical distribution\n",
    "\n",
    "$\\rho(y|x, \\theta) = \\Omega(y|1, f(x;\\theta))$\n",
    "\n",
    "We require that the probability of each y value ($y \\in \\chi$) must be a probability. And they must all sum to 1\n",
    "\n",
    "To avoid forcing $f(x;\\theta)$ to output a probability, we can use a softmax function just like how we previously used the sigmoid function for logistic regression\n",
    "\n",
    "This is also called the multinomial logit\n",
    "\n",
    "$softmax(a) = [\\frac{e^{a_1}}{\\sum_{c'=1}^C e^{a_{c'}}}, ... , ... , \\frac{e^{a_C}}{\\sum_{c'=1}^C e^{a_{c'}}}]$\n",
    "\n",
    "Where C is the number of possible values\n",
    "\n",
    "This operates a bit like an argmax function, in that the classes with the highest log-odds will have the highest softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass logistic regression\n",
    "\n",
    "let $f(x;\\theta) = Wx+b$,\n",
    "\n",
    "where W is a C x D matrix and b is a c-dimensional error vector\n",
    "\n",
    "Fitting this into a cagetorical distribution (how we can model what category a thing is)\n",
    "\n",
    "$\\rho(y|x:\\theta) = Cat(y|softmax(Wx+b))$\n",
    "\n",
    "We can re-write the above as follows for one c value.\n",
    "\n",
    "$\\rho(y=c|x:\\theta) = \\frac{e^{a_c}}{\\sum_{c'=1}^C e^{a_{c'}}}$\n",
    "\n",
    "This is the logistic regression formula. The probability of a given class C is modeled using a linear formula, and collapsed with a softmax function.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
